{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67130e40",
   "metadata": {
    "id": "67130e40"
   },
   "source": [
    "# ML Final Project — Guided Template (CIS 508)\n",
    "\n",
    "**Purpose:**  \n",
    "This notebook is your guide for the final project in Machine Learning in Business. You’ll use it to organize your work and make sure each part of your project is clear and meaningful.\n",
    "\n",
    "Your project should follow four main steps:\n",
    "\n",
    "1. Project Overview – Introduce your topic, explain why it matters, and define your business problems.\n",
    "\n",
    "2. EDA & Data Insights – Explore your dataset, clean it, and highlight key patterns or findings that help you understand the problem.\n",
    "\n",
    "3. Modeling & Evaluation – Build and evaluate your machine learning models. Compare performance across models and explain what you learn from the results (at least, 2 models).\n",
    "\n",
    "4. Executive Summary – Summarize your main insights in plain language. Focus on what your results mean for decision-making or business strategy.\n",
    "\n",
    "Use this structure to keep your analysis focused, your writing organized, and your insights actionable.\n",
    "\n",
    "***Feel free to remove this part of the template when you finalize your project.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tj2z15TYZPoO",
   "metadata": {
    "id": "tj2z15TYZPoO"
   },
   "source": [
    "# Predicting California Housing Prices: A Machine Learning Approach to Real Estate Valuation\n",
    "\n",
    "**One-sentence description:**  \n",
    ">This project uses machine learning models to predict median house values in California based on geographic, demographic, and housing characteristics, helping real estate stakeholders make informed pricing and investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfde32",
   "metadata": {
    "id": "c1dfde32"
   },
   "source": [
    "## Section 1 — Project Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mBOsxuSmjESr",
   "metadata": {
    "id": "mBOsxuSmjESr"
   },
   "source": [
    "In this section, you’ll introduce your project and explain what business problem you’re trying to solve.  \n",
    "Please fill out each part clearly and concisely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hYHAI7s0hmFF",
   "metadata": {
    "id": "hYHAI7s0hmFF"
   },
   "source": [
    "### 1.1 Dataset and Problem Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHLp3oPTjL4q",
   "metadata": {
    "id": "nHLp3oPTjL4q"
   },
   "source": [
    "- Indicate **which dataset** you selected (name and source).\n",
    "> **Dataset**: California Housing Prices  \n",
    "> **Source**: This dataset is based on the 1990 California census data and is commonly used in machine learning courses and competitions. The dataset contains information about housing districts in California, including geographic location, housing characteristics, and demographic information. It is available from various open data platforms and has been widely used for regression analysis and predictive modeling in real estate contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i5cF5Q3lmHB6",
   "metadata": {
    "id": "i5cF5Q3lmHB6"
   },
   "source": [
    "- Describe **the size of the dataset** (number of rows and columns).  \n",
    "> The dataset contains approximately **20,640 observations** (rows) and **10 features** (columns). This provides a substantial sample size for building robust machine learning models while maintaining computational efficiency. The dataset includes one target variable (median_house_value) and nine predictor variables covering geographic location, housing characteristics, and demographic factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_0Y6x6ZymJL2",
   "metadata": {
    "id": "_0Y6x6ZymJL2"
   },
   "source": [
    "- **Introduce all variables**: provide a short description of each key variable and its meaning.\n",
    "> **Target Variable:**\n",
    "> - **median_house_value**: The median house value for households within a block (continuous, in US dollars). This is our target variable for prediction.\n",
    "> \n",
    "> **Predictor Variables:**\n",
    "> - **longitude**: Geographic longitude coordinate of the housing block (continuous, negative values indicate west of prime meridian)\n",
    "> - **latitude**: Geographic latitude coordinate of the housing block (continuous)\n",
    "> - **housing_median_age**: Median age of houses within the block (continuous, in years)\n",
    "> - **total_rooms**: Total number of rooms within the block (continuous)\n",
    "> - **total_bedrooms**: Total number of bedrooms within the block (continuous)\n",
    "> - **population**: Total population residing in the block (continuous)\n",
    "> - **households**: Total number of households in the block (continuous)\n",
    "> - **median_income**: Median income for households within the block (continuous, scaled and capped at 15.0)\n",
    "> - **ocean_proximity**: Proximity to the ocean (categorical: '<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36skGSrIhnh1",
   "metadata": {
    "id": "36skGSrIhnh1"
   },
   "source": [
    "### 1.2 Business Motivation\n",
    "- What is the **business problem** you are trying to solve?  \n",
    "> The business problem is to accurately predict median house values in California housing districts to support strategic decision-making in the real estate market. Accurate house price predictions enable stakeholders to:\n",
    "> - **For Homebuyers**: Make informed purchasing decisions and identify undervalued properties\n",
    "> - **For Real Estate Agents**: Provide accurate pricing guidance to clients and optimize listing strategies\n",
    "> - **For Investors**: Identify profitable investment opportunities and assess market trends\n",
    "> - **For Lenders**: Evaluate property values for mortgage underwriting and risk assessment\n",
    "> - **For Developers**: Make data-driven decisions about where to build and what price points to target\n",
    "> \n",
    "> The challenge lies in understanding which factors most significantly influence house prices and building a reliable predictive model that can generalize to new housing districts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Q4BKyfAmsUE",
   "metadata": {
    "id": "0Q4BKyfAmsUE"
   },
   "source": [
    "- Who are the **stakeholders** involved (e.g., managers, teams, customers)?  \n",
    "> **Primary Stakeholders:**\n",
    "> - **Homebuyers and Home Sellers**: Need accurate market valuations to make informed decisions about buying or selling properties\n",
    "> - **Real Estate Agents and Brokers**: Require reliable price estimates to advise clients and set competitive listing prices\n",
    "> - **Real Estate Investment Firms**: Need accurate valuations to identify investment opportunities and manage portfolios\n",
    "> - **Mortgage Lenders and Banks**: Require property valuations for loan underwriting, risk assessment, and portfolio management\n",
    "> - **Property Developers and Construction Companies**: Need market insights to decide where to build and what price points to target\n",
    "> - **Real Estate Appraisers**: Can use models as a tool to support their professional valuations\n",
    "> - **Government Agencies**: May use predictions for property tax assessments and urban planning decisions\n",
    "> - **Real Estate Technology Platforms** (e.g., Zillow, Redfin): Rely on accurate predictions for their automated valuation models (AVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bmre6FK1mt6v",
   "metadata": {
    "id": "Bmre6FK1mt6v"
   },
   "source": [
    "- What are the **potential benefits or costs of not solving** this problem?\n",
    "> **Benefits of Solving This Problem:**\n",
    "> - **Improved Decision-Making**: Stakeholders can make more informed decisions with accurate price predictions\n",
    "> - **Market Efficiency**: Better price transparency leads to more efficient real estate markets\n",
    "> - **Risk Reduction**: Lenders and investors can better assess and mitigate financial risks\n",
    "> - **Competitive Advantage**: Real estate professionals with superior pricing models gain market advantages\n",
    "> - **Time Savings**: Automated valuations reduce the time needed for manual appraisals\n",
    "> \n",
    "> **Costs of Not Solving This Problem:**\n",
    "> - **Financial Losses**: Overpricing leads to properties sitting on the market; underpricing results in lost revenue\n",
    "> - **Poor Investment Decisions**: Without accurate predictions, investors may choose suboptimal properties or miss opportunities\n",
    "> - **Increased Risk**: Lenders face higher default risks if property values are overestimated\n",
    "> - **Market Inefficiency**: Inaccurate pricing creates market distortions and reduces overall market efficiency\n",
    "> - **Competitive Disadvantage**: Companies without accurate pricing models lose business to competitors with better tools\n",
    "> - **Customer Dissatisfaction**: Homebuyers and sellers lose trust when valuations are consistently inaccurate\n",
    "> - **Regulatory Issues**: Inaccurate valuations can lead to compliance problems and legal issues for financial institutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fe2cc",
   "metadata": {
    "id": "5e0fe2cc"
   },
   "source": [
    "## Section 2 — EDA (Data Understanding & Key Insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uhdyLQ5Jd6Jn",
   "metadata": {
    "id": "uhdyLQ5Jd6Jn"
   },
   "source": [
    "Use this section to **understand your dataset and uncover key business-relevant patterns.** Feel free to include both tables and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oZMl1-SQiIkf",
   "metadata": {
    "id": "oZMl1-SQiIkf"
   },
   "source": [
    "1. Mount your Google Drive.\n",
    "2. Set your working directory (if you’re using the default, no changes are needed).\n",
    "3. Update the file name to match the dataset you selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cpHDWClthA-Z",
   "metadata": {
    "id": "cpHDWClthA-Z"
   },
   "outputs": [],
   "source": [
    "# For local execution (Jupyter Notebook)\n",
    "# If using Google Colab, uncomment the following lines:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/Colab Notebooks\n",
    "\n",
    "# Verify the current working directory\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Read data into jupyter notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read File - California Housing Prices dataset\n",
    "df = pd.read_csv(\"California Housing Prices.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-3nFT9tfkvKA",
   "metadata": {
    "id": "-3nFT9tfkvKA"
   },
   "source": [
    "\n",
    "### 2.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulfQJ2gLm6Rh",
   "metadata": {
    "id": "ulfQJ2gLm6Rh"
   },
   "source": [
    "- Provide a clear, high-level description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DOBCX9UVeAro",
   "metadata": {
    "id": "DOBCX9UVeAro"
   },
   "outputs": [],
   "source": [
    "# Dataset Overview\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset dimensions\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Column information\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COLUMN INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(df.info())\n",
    "\n",
    "# Data types summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA TYPES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(df.dtypes)\n",
    "\n",
    "# Target variable identification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TARGET VARIABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Target Variable (y): median_house_value\")\n",
    "print(\"\\nPredictor Variables (X):\")\n",
    "predictors = [col for col in df.columns if col != 'median_house_value']\n",
    "for i, pred in enumerate(predictors, 1):\n",
    "    print(f\"  {i}. {pred}\")\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(df.describe())\n",
    "\n",
    "# First few rows\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 60)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8l7oQHRCm-uN",
   "metadata": {
    "id": "8l7oQHRCm-uN"
   },
   "source": [
    "### 2.2 Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7vSSdhnnFcJ",
   "metadata": {
    "id": "q7vSSdhnnFcJ"
   },
   "source": [
    "- Diagnose and handle data problems that could affect analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yGbsgnL-gotc",
   "metadata": {
    "id": "yGbsgnL-gotc"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA QUALITY CHECKS\n",
    "# ============================================================\n",
    "\n",
    "# 1. Check for missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\" * 60)\n",
    "missing_values = df.isna().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"✓ No missing values found!\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DUPLICATES CHECK\")\n",
    "print(\"=\" * 60)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "if duplicate_count > 0:\n",
    "    print(\"⚠ Warning: Duplicates found. Consider removing them.\")\n",
    "else:\n",
    "    print(\"✓ No duplicate rows found!\")\n",
    "\n",
    "# 3. Check for impossible/out-of-range values\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA VALIDATION CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for negative values where they shouldn't exist\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "print(\"\\nChecking for negative values in numeric columns:\")\n",
    "for col in numeric_cols:\n",
    "    negative_count = (df[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        print(f\"  ⚠ {col}: {negative_count} negative values found\")\n",
    "    else:\n",
    "        print(f\"  ✓ {col}: No negative values\")\n",
    "\n",
    "# Check ocean_proximity categories\n",
    "print(\"\\nChecking ocean_proximity categories:\")\n",
    "print(f\"  Unique values: {df['ocean_proximity'].unique()}\")\n",
    "print(f\"  Value counts:\\n{df['ocean_proximity'].value_counts()}\")\n",
    "\n",
    "# Check for extreme outliers in key variables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OUTLIER DETECTION (Using IQR Method)\")\n",
    "print(\"=\" * 60)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "key_variables = ['median_house_value', 'median_income', 'housing_median_age', 'total_rooms']\n",
    "for var in key_variables:\n",
    "    if var in df.columns:\n",
    "        outliers, lower, upper = detect_outliers_iqr(df, var)\n",
    "        outlier_pct = (len(outliers) / len(df)) * 100\n",
    "        print(f\"\\n{var}:\")\n",
    "        print(f\"  Lower bound: {lower:.2f}, Upper bound: {upper:.2f}\")\n",
    "        print(f\"  Outliers: {len(outliers)} ({outlier_pct:.2f}%)\")\n",
    "\n",
    "# Handle missing values in total_bedrooms (if any)\n",
    "if df['total_bedrooms'].isna().sum() > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HANDLING MISSING VALUES\")\n",
    "    print(\"=\" * 60)\n",
    "    # Impute missing bedrooms with median\n",
    "    median_bedrooms = df['total_bedrooms'].median()\n",
    "    df['total_bedrooms'].fillna(median_bedrooms, inplace=True)\n",
    "    print(f\"✓ Imputed {df['total_bedrooms'].isna().sum()} missing values in total_bedrooms with median: {median_bedrooms}\")\n",
    "\n",
    "# Remove duplicates if found\n",
    "if duplicate_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"\\n✓ Removed {duplicate_count} duplicate rows\")\n",
    "    print(f\"  New dataset shape: {df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA QUALITY CHECK COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2A0EPeeOnCJx",
   "metadata": {
    "id": "2A0EPeeOnCJx"
   },
   "source": [
    "### 2.3 Descriptive Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pkE--1VVnIMo",
   "metadata": {
    "id": "pkE--1VVnIMo"
   },
   "source": [
    "- The goal of data exploration is to **discover patterns, relationships, and stories** within your data before modeling. This step is not about following fixed rules — it’s about curiosity, creativity, and developing intuition. You are encouraged to **freely explore** the data in ways that make sense for your project — visualize, compare, or test relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nvf3DURMmAUc",
   "metadata": {
    "id": "nvf3DURMmAUc"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DESCRIPTIVE EXPLORATIONS\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style for better-looking plots\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. Distribution of Target Variable\n",
    "print(\"=\" * 60)\n",
    "print(\"1. TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['median_house_value'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Median House Value ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Median House Values')\n",
    "axes[0].axvline(df['median_house_value'].median(), color='red', linestyle='--', \n",
    "                label=f'Median: ${df[\"median_house_value\"].median():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df['median_house_value'], vert=True)\n",
    "axes[1].set_ylabel('Median House Value ($)')\n",
    "axes[1].set_title('Box Plot of Median House Values')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTarget Variable Statistics:\")\n",
    "print(df['median_house_value'].describe())\n",
    "\n",
    "# 2. Geographic Distribution\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. GEOGRAPHIC DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(df['longitude'], df['latitude'], \n",
    "                    c=df['median_house_value'], cmap='viridis', \n",
    "                    alpha=0.6, s=20, edgecolors='black', linewidth=0.5)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Geographic Distribution of House Values in California')\n",
    "plt.colorbar(scatter, label='Median House Value ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation Analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate correlation matrix for numeric variables\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Numeric Variables', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target variable\n",
    "print(\"\\nCorrelations with Median House Value:\")\n",
    "target_corr = correlation_matrix['median_house_value'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# 4. Distribution of Key Predictor Variables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. DISTRIBUTION OF KEY PREDICTOR VARIABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Median Income\n",
    "axes[0, 0].hist(df['median_income'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_xlabel('Median Income (scaled)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Median Income')\n",
    "\n",
    "# Housing Median Age\n",
    "axes[0, 1].hist(df['housing_median_age'], bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_xlabel('Housing Median Age (years)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Housing Median Age')\n",
    "\n",
    "# Total Rooms\n",
    "axes[1, 0].hist(df['total_rooms'], bins=50, edgecolor='black', alpha=0.7, color='salmon')\n",
    "axes[1, 0].set_xlabel('Total Rooms')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Total Rooms')\n",
    "\n",
    "# Population\n",
    "axes[1, 1].hist(df['population'], bins=50, edgecolor='black', alpha=0.7, color='plum')\n",
    "axes[1, 1].set_xlabel('Population')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Population')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Relationship Between Key Variables and Target\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. RELATIONSHIPS WITH TARGET VARIABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Median Income vs House Value\n",
    "axes[0, 0].scatter(df['median_income'], df['median_house_value'], alpha=0.5, s=10)\n",
    "axes[0, 0].set_xlabel('Median Income (scaled)')\n",
    "axes[0, 0].set_ylabel('Median House Value ($)')\n",
    "axes[0, 0].set_title('Median Income vs House Value')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Housing Age vs House Value\n",
    "axes[0, 1].scatter(df['housing_median_age'], df['median_house_value'], alpha=0.5, s=10)\n",
    "axes[0, 1].set_xlabel('Housing Median Age (years)')\n",
    "axes[0, 1].set_ylabel('Median House Value ($)')\n",
    "axes[0, 1].set_title('Housing Age vs House Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Total Rooms vs House Value\n",
    "axes[1, 0].scatter(df['total_rooms'], df['median_house_value'], alpha=0.5, s=10)\n",
    "axes[1, 0].set_xlabel('Total Rooms')\n",
    "axes[1, 0].set_ylabel('Median House Value ($)')\n",
    "axes[1, 0].set_title('Total Rooms vs House Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Population vs House Value\n",
    "axes[1, 1].scatter(df['population'], df['median_house_value'], alpha=0.5, s=10)\n",
    "axes[1, 1].set_xlabel('Population')\n",
    "axes[1, 1].set_ylabel('Median House Value ($)')\n",
    "axes[1, 1].set_title('Population vs House Value')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Ocean Proximity Analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. OCEAN PROXIMITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Box plot by ocean proximity\n",
    "df.boxplot(column='median_house_value', by='ocean_proximity', ax=axes[0])\n",
    "axes[0].set_xlabel('Ocean Proximity')\n",
    "axes[0].set_ylabel('Median House Value ($)')\n",
    "axes[0].set_title('House Values by Ocean Proximity')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar plot of average house values\n",
    "ocean_avg = df.groupby('ocean_proximity')['median_house_value'].mean().sort_values(ascending=False)\n",
    "ocean_avg.plot(kind='bar', ax=axes[1], color='teal', edgecolor='black')\n",
    "axes[1].set_xlabel('Ocean Proximity')\n",
    "axes[1].set_ylabel('Average Median House Value ($)')\n",
    "axes[1].set_title('Average House Values by Ocean Proximity')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage House Values by Ocean Proximity:\")\n",
    "print(ocean_avg)\n",
    "\n",
    "# 7. Derived Variables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"7. DERIVED VARIABLES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create derived variables\n",
    "df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "df['population_per_household'] = df['population'] / df['households']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Rooms per household vs House Value\n",
    "axes[0].scatter(df['rooms_per_household'], df['median_house_value'], alpha=0.5, s=10)\n",
    "axes[0].set_xlabel('Rooms per Household')\n",
    "axes[0].set_ylabel('Median House Value ($)')\n",
    "axes[0].set_title('Rooms per Household vs House Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bedrooms per room vs House Value\n",
    "axes[1].scatter(df['bedrooms_per_room'], df['median_house_value'], alpha=0.5, s=10)\n",
    "axes[1].set_xlabel('Bedrooms per Room')\n",
    "axes[1].set_ylabel('Median House Value ($)')\n",
    "axes[1].set_title('Bedrooms per Room vs House Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Population per household vs House Value\n",
    "axes[2].scatter(df['population_per_household'], df['median_house_value'], alpha=0.5, s=10)\n",
    "axes[2].set_xlabel('Population per Household')\n",
    "axes[2].set_ylabel('Median House Value ($)')\n",
    "axes[2].set_title('Population per Household vs House Value')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelations of Derived Variables with House Value:\")\n",
    "derived_vars = ['rooms_per_household', 'bedrooms_per_room', 'population_per_household']\n",
    "for var in derived_vars:\n",
    "    corr = df[var].corr(df['median_house_value'])\n",
    "    print(f\"  {var}: {corr:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DESCRIPTIVE EXPLORATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kcmZPfwDnDWY",
   "metadata": {
    "id": "kcmZPfwDnDWY"
   },
   "source": [
    "### 2.4 Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JuOIcAocnJs2",
   "metadata": {
    "id": "JuOIcAocnJs2"
   },
   "source": [
    "In this section, summarize **what you discovered** from your data exploration (Section 2.3).  \n",
    "Your goal is to **translate observations into insights** that connect back to your business question.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jemt483WnR2D",
   "metadata": {
    "id": "jemt483WnR2D"
   },
   "source": [
    "**Key Insights from Data Exploration:**\n",
    "\n",
    "> **1. Median Income is the Strongest Predictor**\n",
    "> - Median income shows the strongest positive correlation (approximately 0.69) with median house value, indicating that income levels are the most influential factor in determining house prices. This makes intuitive business sense: higher-income areas can support higher property values. The relationship appears relatively linear, though there's a notable cap at higher income levels (scaled value of 15.0), suggesting potential data preprocessing or economic constraints.\n",
    "\n",
    "> **2. Geographic Location Strongly Influences House Values**\n",
    "> - The geographic visualization reveals clear spatial patterns: coastal areas (particularly near the ocean and bays) show significantly higher house values compared to inland regions. Ocean proximity analysis shows that properties near the ocean command premium prices, with ISLAND locations having the highest average values, followed by NEAR OCEAN and NEAR BAY categories. This geographic premium is a critical factor for real estate stakeholders to consider when pricing properties or making investment decisions.\n",
    "\n",
    "> **3. Derived Variables Reveal Important Housing Characteristics**\n",
    "> - The derived variable \"rooms_per_household\" shows a meaningful positive correlation with house values, suggesting that larger homes (more rooms per household) command higher prices. This insight helps explain why total_rooms alone may not be as predictive—it's the ratio relative to households that matters more. Additionally, the analysis reveals that housing age has a complex relationship with value, with both very new and very old properties showing different value patterns.\n",
    "\n",
    "> **4. Data Quality and Distribution Characteristics**\n",
    "> - The dataset is generally clean with minimal missing values (primarily in total_bedrooms, which was handled through imputation). However, the target variable (median_house_value) shows a right-skewed distribution with a notable cap at $500,000, suggesting potential data preprocessing or economic constraints. The presence of outliers in variables like total_rooms and population indicates the need for careful feature engineering and potentially robust modeling approaches that can handle these extreme values.\n",
    "\n",
    "> **5. Unexpected Patterns and Business Implications**\n",
    "> - An interesting finding is that population density (population_per_household) shows a negative correlation with house values in some areas, suggesting that overcrowded areas may actually have lower property values—a counterintuitive finding that could indicate urban vs. suburban preferences or quality-of-life factors. Additionally, the correlation analysis reveals that some variables (like total_bedrooms and total_rooms) are highly correlated with each other, suggesting potential multicollinearity that should be addressed in modeling through feature selection or dimensionality reduction techniques.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f3ea66",
   "metadata": {
    "id": "b3f3ea66"
   },
   "source": [
    "## Section 3 — Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HDSCfeLYqojl",
   "metadata": {
    "id": "HDSCfeLYqojl"
   },
   "source": [
    "### 3.1 Problem Type and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xp5uBGx_qvm4",
   "metadata": {
    "id": "xp5uBGx_qvm4"
   },
   "source": [
    "- State whether this is a **supervised** or **unsupervised** task.  \n",
    "- List the **features** you selected and explain briefly why.  \n",
    "- How did you split or sample your data (e.g., train/test, cross-validation)?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SHYRxtfZueim",
   "metadata": {
    "id": "SHYRxtfZueim"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.1 PROBLEM TYPE AND SETUP - CODE IMPLEMENTATION\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SECTION 3.1: DATA PREPARATION FOR MODELING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we have the derived variables from Section 2\n",
    "if 'rooms_per_household' not in df.columns:\n",
    "    df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "    df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "    df['population_per_household'] = df['population'] / df['households']\n",
    "\n",
    "# Handle any infinite or NaN values in derived variables\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(df.median())\n",
    "\n",
    "# Prepare features\n",
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'longitude', 'latitude', 'housing_median_age',\n",
    "    'total_rooms', 'total_bedrooms', 'population', 'households',\n",
    "    'median_income',\n",
    "    'rooms_per_household', 'bedrooms_per_room', 'population_per_household',\n",
    "    'ocean_proximity'\n",
    "]\n",
    "\n",
    "# Separate features and target\n",
    "X = df[feature_columns].copy()\n",
    "y = df['median_house_value'].copy()\n",
    "\n",
    "print(f\"\\nFeatures selected: {len(feature_columns)}\")\n",
    "print(f\"Feature names: {feature_columns}\")\n",
    "\n",
    "# Handle categorical variable (ocean_proximity)\n",
    "print(\"\\nEncoding categorical variable: ocean_proximity\")\n",
    "le = LabelEncoder()\n",
    "X['ocean_proximity_encoded'] = le.fit_transform(X['ocean_proximity'])\n",
    "X = X.drop('ocean_proximity', axis=1)\n",
    "\n",
    "print(f\"Encoded categories: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Split the data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"Train/Test split: {X_train.shape[0]/(X_train.shape[0]+X_test.shape[0])*100:.1f}% / {X_test.shape[0]/(X_train.shape[0]+X_test.shape[0])*100:.1f}%\")\n",
    "\n",
    "# Scale features (important for linear regression and distance-based models)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\" * 60)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"Scaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Display target variable statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TARGET VARIABLE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set - Median House Value:\")\n",
    "print(f\"  Mean: ${y_train.mean():,.2f}\")\n",
    "print(f\"  Median: ${y_train.median():,.2f}\")\n",
    "print(f\"  Std: ${y_train.std():,.2f}\")\n",
    "print(f\"\\nTest set - Median House Value:\")\n",
    "print(f\"  Mean: ${y_test.mean():,.2f}\")\n",
    "print(f\"  Median: ${y_test.median():,.2f}\")\n",
    "print(f\"  Std: ${y_test.std():,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA PREPARATION COMPLETE - READY FOR MODELING\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OKTWyI3Zqps8",
   "metadata": {
    "id": "OKTWyI3Zqps8"
   },
   "source": [
    "### 3.2 Model Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eYgUH_yaqxzQ",
   "metadata": {
    "id": "eYgUH_yaqxzQ"
   },
   "source": [
    "- List at least **two models** you applied (e.g., Logistic Regression vs Random Forest).\n",
    "- Perform evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u6cBfqo7ufJG",
   "metadata": {
    "id": "u6cBfqo7ufJG"
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# SECTION 3.2 — MODEL IMPLEMENTATIONS AND EVALUATION\n",
    "# =============================================================\n",
    "\n",
    "# Store results for comparison\n",
    "model_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# MODEL 1: LINEAR REGRESSION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "lr_train_r2 = r2_score(y_train, y_train_pred_lr)\n",
    "lr_test_r2 = r2_score(y_test, y_test_pred_lr)\n",
    "lr_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))\n",
    "lr_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))\n",
    "lr_train_mae = mean_absolute_error(y_train, y_train_pred_lr)\n",
    "lr_test_mae = mean_absolute_error(y_test, y_test_pred_lr)\n",
    "\n",
    "# Store results\n",
    "model_results['Linear Regression'] = {\n",
    "    'train_r2': lr_train_r2,\n",
    "    'test_r2': lr_test_r2,\n",
    "    'train_rmse': lr_train_rmse,\n",
    "    'test_rmse': lr_test_rmse,\n",
    "    'train_mae': lr_train_mae,\n",
    "    'test_mae': lr_test_mae,\n",
    "    'model': lr_model\n",
    "}\n",
    "\n",
    "print(\"\\nTraining Performance:\")\n",
    "print(f\"  R² Score: {lr_train_r2:.4f}\")\n",
    "print(f\"  RMSE: ${lr_train_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${lr_train_mae:,.2f}\")\n",
    "\n",
    "print(\"\\nTest Performance:\")\n",
    "print(f\"  R² Score: {lr_test_r2:.4f}\")\n",
    "print(f\"  RMSE: ${lr_test_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${lr_test_mae:,.2f}\")\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(\"\\nTop 5 Most Important Features (by absolute coefficient):\")\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': lr_model.coef_\n",
    "})\n",
    "feature_importance_lr['abs_coefficient'] = np.abs(feature_importance_lr['coefficient'])\n",
    "feature_importance_lr = feature_importance_lr.sort_values('abs_coefficient', ascending=False)\n",
    "print(feature_importance_lr.head())\n",
    "\n",
    "# Visualization: Residuals plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(y_test_pred_lr, y_test - y_test_pred_lr, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted House Value ($)')\n",
    "axes[0].set_ylabel('Residuals ($)')\n",
    "axes[0].set_title('Linear Regression: Residuals vs Predicted')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[1].scatter(y_test, y_test_pred_lr, alpha=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual House Value ($)')\n",
    "axes[1].set_ylabel('Predicted House Value ($)')\n",
    "axes[1].set_title('Linear Regression: Actual vs Predicted')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel 1 Summary:\")\n",
    "print(\"  Strengths: Interpretable coefficients, fast training, no hyperparameters to tune\")\n",
    "print(\"  Weaknesses: Assumes linear relationships, may not capture complex patterns\")\n",
    "print(\"  Interpretability: High - coefficients show feature importance directly\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL 2: RANDOM FOREST REGRESSOR\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL 2: RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model (using unscaled data as Random Forest doesn't require scaling)\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "rf_train_r2 = r2_score(y_train, y_train_pred_rf)\n",
    "rf_test_r2 = r2_score(y_test, y_test_pred_rf)\n",
    "rf_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
    "rf_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "rf_train_mae = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "rf_test_mae = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "\n",
    "# Store results\n",
    "model_results['Random Forest'] = {\n",
    "    'train_r2': rf_train_r2,\n",
    "    'test_r2': rf_test_r2,\n",
    "    'train_rmse': rf_train_rmse,\n",
    "    'test_rmse': rf_test_rmse,\n",
    "    'train_mae': rf_train_mae,\n",
    "    'test_mae': rf_test_mae,\n",
    "    'model': rf_model\n",
    "}\n",
    "\n",
    "print(\"\\nTraining Performance:\")\n",
    "print(f\"  R² Score: {rf_train_r2:.4f}\")\n",
    "print(f\"  RMSE: ${rf_train_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${rf_train_mae:,.2f}\")\n",
    "\n",
    "print(\"\\nTest Performance:\")\n",
    "print(f\"  R² Score: {rf_test_r2:.4f}\")\n",
    "print(f\"  RMSE: ${rf_test_rmse:,.2f}\")\n",
    "print(f\"  MAE: ${rf_test_mae:,.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance_rf = feature_importance_rf.sort_values('importance', ascending=False)\n",
    "print(feature_importance_rf.head(10))\n",
    "\n",
    "# Visualization: Feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance_rf.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest: Top 10 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Residuals and predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(y_test_pred_rf, y_test - y_test_pred_rf, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted House Value ($)')\n",
    "axes[0].set_ylabel('Residuals ($)')\n",
    "axes[0].set_title('Random Forest: Residuals vs Predicted')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[1].scatter(y_test, y_test_pred_rf, alpha=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual House Value ($)')\n",
    "axes[1].set_ylabel('Predicted House Value ($)')\n",
    "axes[1].set_title('Random Forest: Actual vs Predicted')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel 2 Summary:\")\n",
    "print(\"  Strengths: Handles non-linear relationships, captures feature interactions, robust to outliers\")\n",
    "print(\"  Weaknesses: Less interpretable than linear regression, can overfit with many trees\")\n",
    "print(\"  Interpretability: Medium - feature importance available but individual predictions harder to explain\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL COMPARISON SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest'],\n",
    "    'Train R²': [lr_train_r2, rf_train_r2],\n",
    "    'Test R²': [lr_test_r2, rf_test_r2],\n",
    "    'Train RMSE': [lr_train_rmse, rf_train_rmse],\n",
    "    'Test RMSE': [lr_test_rmse, rf_test_rmse],\n",
    "    'Train MAE': [lr_train_mae, rf_train_mae],\n",
    "    'Test MAE': [lr_test_mae, rf_test_mae]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison Table:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['R² Score', 'RMSE ($)', 'MAE ($)']\n",
    "train_values = [[lr_train_r2, rf_train_r2], [lr_train_rmse, rf_train_rmse], [lr_train_mae, rf_train_mae]]\n",
    "test_values = [[lr_test_r2, rf_test_r2], [lr_test_rmse, rf_test_rmse], [lr_test_mae, rf_test_mae]]\n",
    "\n",
    "for i, (metric, train_vals, test_vals) in enumerate(zip(metrics, train_values, test_values)):\n",
    "    x = np.arange(len(['Linear Regression', 'Random Forest']))\n",
    "    width = 0.35\n",
    "    axes[i].bar(x - width/2, train_vals, width, label='Train', alpha=0.8)\n",
    "    axes[i].bar(x + width/2, test_vals, width, label='Test', alpha=0.8)\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_title(f'Model Comparison: {metric}')\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(['Linear Regression', 'Random Forest'])\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J3Tb5M4Nqq15",
   "metadata": {
    "id": "J3Tb5M4Nqq15"
   },
   "source": [
    "### 3.3 Model Comparisons & Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0VlhAQBmq3KJ",
   "metadata": {
    "id": "0VlhAQBmq3KJ"
   },
   "source": [
    "**Which model performed better overall, and why?**\n",
    "\n",
    "Based on the evaluation metrics, the **Random Forest Regressor** performed better overall than Linear Regression. The Random Forest model achieved a higher R² score on the test set, indicating it explains more variance in house prices. It also achieved lower RMSE and MAE values, meaning its predictions are closer to actual values on average. This superior performance is expected because Random Forest can capture non-linear relationships and feature interactions that Linear Regression cannot. The housing market involves complex interactions between geographic location, income levels, and housing characteristics that are better modeled by ensemble methods.\n",
    "\n",
    "**Did any feature show a strong influence on the prediction?**\n",
    "\n",
    "Yes, both models consistently identified **median_income** as the most influential feature, which aligns with our EDA findings showing a 0.69 correlation. The Random Forest feature importance analysis confirms that median_income dominates predictions, followed by geographic features (longitude, latitude) and the derived variable rooms_per_household. Ocean proximity (encoded) also shows significant importance, reinforcing the coastal premium effect we discovered in EDA. The consistency between EDA insights and model feature importance validates our exploratory analysis.\n",
    "\n",
    "**Were there trade-offs (e.g., interpretability vs accuracy)?**\n",
    "\n",
    "Yes, there is a clear trade-off between interpretability and accuracy:\n",
    "\n",
    "- **Linear Regression**: High interpretability (coefficients directly show feature effects) but lower accuracy. Each coefficient tells us exactly how a one-unit change in a feature affects house value, making it easy to explain predictions to stakeholders. However, it assumes linear relationships and misses complex patterns.\n",
    "\n",
    "- **Random Forest**: Higher accuracy and better predictive performance but lower interpretability. While we can see feature importance rankings, explaining individual predictions is more complex. The model captures non-linear relationships and interactions but operates as a \"black box\" for specific predictions.\n",
    "\n",
    "For business deployment, the choice depends on the use case: Linear Regression is better for regulatory compliance and stakeholder communication, while Random Forest is better for maximizing prediction accuracy in automated systems.\n",
    "\n",
    "**Limitations, surprises, and ideas for improvement:**\n",
    "\n",
    "**Limitations:**\n",
    "1. Both models show some overfitting (higher train than test R²), particularly Random Forest, suggesting we may need regularization or cross-validation tuning.\n",
    "2. The $500,000 cap on house values limits model performance on high-value properties.\n",
    "3. The 1990 data may not reflect current market dynamics, geographic shifts, or economic changes.\n",
    "\n",
    "**Surprises:**\n",
    "1. The derived variable rooms_per_household showed stronger predictive power than raw total_rooms, validating our feature engineering approach.\n",
    "2. Linear Regression performed reasonably well despite the non-linear relationships, suggesting income and location dominate predictions in a relatively linear way.\n",
    "\n",
    "**Ideas for improvement:**\n",
    "1. **Hyperparameter tuning**: Use GridSearchCV or RandomizedSearchCV to optimize Random Forest parameters (n_estimators, max_depth, min_samples_split) to reduce overfitting.\n",
    "2. **Additional models**: Try Gradient Boosting (XGBoost, LightGBM) which often outperforms Random Forest on structured data.\n",
    "3. **Feature engineering**: Create interaction terms (e.g., income × ocean_proximity) to explicitly model the interaction effects.\n",
    "4. **Ensemble methods**: Combine predictions from multiple models to potentially improve accuracy.\n",
    "5. **Address data limitations**: Obtain more recent data, remove the $500K cap, and incorporate additional features like school ratings, crime statistics, and transportation access.\n",
    "6. **Cross-validation**: Implement k-fold cross-validation for more robust performance estimates.\n",
    "7. **Residual analysis**: Investigate patterns in residuals to identify systematic prediction errors and areas for model improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce178eec",
   "metadata": {
    "id": "ce178eec"
   },
   "source": [
    "## Section 4 — Executive Summary & Business Implications\n",
    "The Executive Summary must not exceed ***500*** words.\n",
    "\n",
    "### 4.1 Main Findings\n",
    "\n",
    "**Primary Insight**: Median income is the strongest predictor of house values in California (correlation 0.69), with geographic location (particularly ocean proximity) serving as the second most influential factor, creating a clear coastal premium where properties near the ocean command significantly higher prices than inland regions.\n",
    "\n",
    "Our exploratory data analysis reveals three critical patterns driving property valuations. First, median income fundamentally determines what buyers can afford in each district. Second, geographic location creates substantial value differentials, with ocean proximity categories showing average price differences exceeding $100,000 between coastal and inland properties. Properties on islands command the highest premiums, followed by those near the ocean and bays, while inland properties represent the most affordable segment. Third, housing characteristics matter when normalized by household count—specifically, rooms per household shows meaningful predictive power, suggesting that larger homes relative to household size drive higher valuations.\n",
    "\n",
    "The data also reveals nuances: housing age has a complex relationship with value, and population density shows unexpected negative correlations in some areas, potentially reflecting quality-of-life preferences. These findings suggest that real estate valuation models must account for economic capacity (income) and location desirability (geography) as primary drivers.\n",
    "\n",
    "### 4.2 Business Recommendations\n",
    "\n",
    "**1. Develop Location-Based Pricing Strategies**\n",
    "Real estate agents and developers should prioritize coastal and near-ocean properties for premium pricing strategies, while inland properties require competitive pricing approaches. Marketing materials should emphasize ocean proximity as a value driver, and investment firms should allocate portfolio resources accordingly.\n",
    "\n",
    "**2. Use Income Data for Market Segmentation**\n",
    "Lenders and real estate professionals should leverage median income data to segment markets and predict price ranges. Higher-income districts consistently support higher property values, making income levels a reliable indicator for pricing strategies, loan underwriting risk assessment, and investment targeting.\n",
    "\n",
    "**3. Create Derived Metrics for Better Predictions**\n",
    "Instead of using raw totals like total_rooms or total_bedrooms, stakeholders should calculate ratios such as rooms_per_household, which better reflect housing quality and correlate more strongly with values. Property valuation models should incorporate these normalized metrics.\n",
    "\n",
    "**4. Implement Geographic Visualization Tools**\n",
    "Real estate technology platforms should enhance their automated valuation models with geographic visualization capabilities that highlight spatial price patterns, helping users understand location-based value drivers.\n",
    "\n",
    "### 4.3 Caveats and Next Steps\n",
    "\n",
    "**Limitations and Risks**: The dataset is based on 1990 census data, which may not reflect current market conditions over three decades. The target variable shows a hard cap at $500,000, suggesting data preprocessing that may limit model performance on high-value properties. Additionally, outliers in variables like total_rooms and population require robust modeling approaches.\n",
    "\n",
    "**Next Steps**: To strengthen these findings, we recommend: (1) implementing machine learning models (linear regression, random forest, gradient boosting) to quantify predictive relationships; (2) obtaining more recent data to assess whether patterns persist in contemporary markets; (3) incorporating additional features such as school district ratings, crime statistics, and transportation access; (4) developing real-time model monitoring systems; and (5) conducting sensitivity analyses on key predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O5qu5RmDIUHX",
   "metadata": {
    "id": "O5qu5RmDIUHX"
   },
   "source": [
    "## Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-HWy4a52a6Wg",
   "metadata": {
    "id": "-HWy4a52a6Wg"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html \"Final_Project_Template_gc.ipynb\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
